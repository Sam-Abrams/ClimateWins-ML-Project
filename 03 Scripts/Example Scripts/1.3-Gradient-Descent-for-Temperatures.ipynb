{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb06020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode\n",
    "from plotly.offline import plot, iplot\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b992c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a path to where your data is stored.\n",
    "path = r'/Users/yourname/Datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9142c894",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/yourname/Datasets/DATASET.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Read in the European weather data.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m climate \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATASET.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m climate\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/yourname/Datasets/DATASET.csv'"
     ]
    }
   ],
   "source": [
    "#Read in the European weather data.\n",
    "climate = pd.read_csv(os.path.join(path, 'DATASET.csv'))\n",
    "climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce to just the mean temperatures\n",
    "df = climate[['DATE', 'MONTH','BASEL_temp_mean',\n",
    " 'BELGRADE_temp_mean',\n",
    " 'BUDAPEST_temp_mean',\n",
    " 'DEBILT_temp_mean',\n",
    " 'DUSSELDORF_temp_mean',\n",
    " 'GDANSK_temp_mean',\n",
    " 'HEATHROW_temp_mean',\n",
    " 'KASSEL_temp_mean',\n",
    " 'LJUBLJANA_temp_mean',\n",
    " 'MAASTRICHT_temp_mean',\n",
    " 'MADRID_temp_mean',\n",
    " 'MUNCHENB_temp_mean',\n",
    " 'OSLO_temp_mean',\n",
    " 'ROMA_temp_mean',\n",
    " 'SONNBLICK_temp_mean',\n",
    " 'STOCKHOLM_temp_mean',\n",
    " 'TOURS_temp_mean',\n",
    " 'VALENTIA_temp_mean']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28670a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a856771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You'll need to reduce the dataset to only one year of data. Analyze and pick which year you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00dc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the DATE and MONTH data as those numbers are not scaled with the rest.\n",
    "notemp = df.drop(['DATE','MONTH'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at a whisker plot of the data to see variations in temperatures\n",
    "notemp.boxplot(figsize=(15,15))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739173ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce your dataset to a single year\n",
    "dfyear = df[df['DATE'].astype(str).str.contains('1960')] #<-----INSERT YEAR HERE\n",
    "dfyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfyear.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick which weather station you want to use. Below is a 3D visualization of the temperatures for that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the DATE and MONTH data as those numbers are not scaled with the rest.\n",
    "notempyear = dfyear.drop(['DATE','MONTH'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b15968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ALL weather data for all stations for a year \n",
    "\n",
    "#X = weather station\n",
    "#Y = day of the year\n",
    "#Z = temperature\n",
    "\n",
    "#you can click/hold in the graph below to rotate!\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=notempyear.values)])\n",
    "fig.update_layout(title='Temperatures over time', autosize=False,\n",
    "                  width=600, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to make an index for the year. Create a set of data from 1 to 365 (or to 366 if it's a leap year!)\n",
    "#We'll scale this by 100 as the index is made. This will help teh gradient descent converge 366 = 3.66\n",
    "\n",
    "i = np.arange(0.01,3.66,0.01) #<---needs to be one GREATER than the total number of days\n",
    "index = pd.DataFrame(data = i, columns = ['index'])\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = dfyear.shape[0]\n",
    "n_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f19f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will translate your chosen weather data into the X and y datasets needed for the optimization function.\n",
    "\n",
    "X=index.to_numpy().reshape(n_rows,1)\n",
    "#Represent x_0 as a vector of 1s for vector computation\n",
    "ones = np.ones((n_rows,1))\n",
    "X = np.concatenate((ones, X), axis=1)\n",
    "y=dfyear['STATION_temp_mean'].to_numpy().reshape(n_rows,1) #<----INSERT WEATHER STATION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd319406",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at one year of temperature data over time\n",
    "plt.scatter(x=index['index'], y=dfyear['STATION_temp_mean']) #<----INSERT WEATHER STATION HERE\n",
    "plt.xlabel('X'); plt.ylabel('y');\n",
    "plt.title('Input dataset');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ab6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the min temperature? (Note gradient descent is not actually finding this number)\n",
    "dfyear['STATION_temp_mean'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb51050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the max temperature? (Note gradient descent is not actually finding this number)\n",
    "dfyear['STATION_temp_mean'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This computes the loss function for the gradiant descent. DO NOT CHANGE!\n",
    "\n",
    "def compute_cost(X, y, theta=np.array([[0],[0]])):\n",
    "    \"\"\"Given covariate matrix X, the prediction results y and coefficients theta\n",
    "    compute the loss\"\"\"\n",
    "    \n",
    "    m = len(y)\n",
    "    J=0 # initialize loss to zero\n",
    "    \n",
    "    # reshape theta\n",
    "    theta=theta.reshape(2,1)\n",
    "    \n",
    "    # calculate the hypothesis - y_hat\n",
    "    h_x = np.dot(X,theta)\n",
    "    #print(h_x)\n",
    "    \n",
    "    # subtract y from y_hat, square and sum\n",
    "    error_term = sum((h_x - y)**2)\n",
    "    \n",
    "    # divide by twice the number of samples - standard practice.\n",
    "    loss = error_term/(2*m)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aec9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cost(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38627745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the gradiant descent function. DO NOT CHANGE!\n",
    "\n",
    "def gradient_descent(X, y, theta=np.array([[0],[0]]),\n",
    "                    alpha=0.01, num_iterations=1500):\n",
    "    \"\"\"\n",
    "    Solve for theta using Gradient Descent optimiztion technique. \n",
    "    Alpha is the learning rate\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    J_history = []\n",
    "    theta0_history = []\n",
    "    theta1_history = []\n",
    "    theta = theta.reshape(2,1)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        error = (np.dot(X, theta) - y)\n",
    "        \n",
    "        term0 = (alpha/m) * sum(error* X[:,0].reshape(m,1))\n",
    "        term1 = (alpha/m) * sum(error* X[:,1].reshape(m,1))\n",
    "        \n",
    "        # update theta\n",
    "        term_vector = np.array([[term0],[term1]])\n",
    "        #print(term_vector)\n",
    "        theta = theta - term_vector.reshape(2,1)\n",
    "        \n",
    "        # store history values\n",
    "        theta0_history.append(theta[0].tolist()[0])\n",
    "        theta1_history.append(theta[1].tolist()[0])\n",
    "        J_history.append(compute_cost(X,y,theta).tolist()[0])\n",
    "        \n",
    "    return (theta, J_history, theta0_history, theta1_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615cda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#This runs your data through a gradiant descent for the starting conditions in 'theta_init.'\n",
    "#You will need to adjust these numbers\n",
    "\n",
    "num_iterations=10 #<---Decide how many iterations you need. Start small and work up. Over 10,000 iterations will take a few seconds.\n",
    "theta_init=np.array([[-10],[-5]]) #<---this is where you put the guess for [theta0], [theta1]. Start with 1 and 1.\n",
    "alpha=0.1 #<---Decide what your step size is. Try values between 0.1 and 0.00001. You will need to adjust your iterations.\n",
    "#If your solution is not converging, try a smaller step size.\n",
    "theta, J_history, theta0_history, theta1_history = gradient_descent(X,y, theta_init,\n",
    "                                                                   alpha, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f53115",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffa7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will plot your loss, theta0, and theta1.If the result looks like a straight line, it's not converging on an answer!\n",
    "#Your loss (red) should be trending toward 0.\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# plot thetas over time\n",
    "color='tab:blue'\n",
    "ax1.plot(theta0_history, label='$\\\\theta_{0}$', linestyle='--', color=color)\n",
    "ax1.plot(theta1_history, label='$\\\\theta_{1}$', linestyle='-', color=color)\n",
    "# ax1.legend()\n",
    "ax1.set_xlabel('Iterations'); ax1.set_ylabel('$\\\\theta$', color=color);\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# plot loss function over time\n",
    "color='tab:red'\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(J_history, label='Loss function', color=color)\n",
    "ax2.set_title('Values of $\\\\theta$ and $J(\\\\theta)$ over iterations')\n",
    "ax2.set_ylabel('Loss: $J(\\\\theta)$', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# ax2.legend();\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ed155",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# theta range\n",
    "theta0_vals = np.linspace(-10,10,100) #Look in the chart above for the limits of where theta0 and theta1 appear.\n",
    "theta1_vals = np.linspace(-10,10,100) #Put those values as the first two \"linspace\" numbers in these lines\n",
    "                                      #Select with large margins, maybe +/- 10\n",
    "J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))\n",
    "\n",
    "# compute cost for each combination of theta\n",
    "c1=0; c2=0\n",
    "for i in theta0_vals:\n",
    "    for j in theta1_vals:\n",
    "        t = np.array([i, j])\n",
    "        J_vals[c1][c2] = compute_cost(X, y, t.transpose()).tolist()[0]\n",
    "        c2=c2+1\n",
    "    c1=c1+1\n",
    "    c2=0 # reinitialize to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097228b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This figure shows the loss function.\n",
    "\n",
    "#X = Theta0\n",
    "#Y - Theta1\n",
    "#Z = Loss\n",
    "#Find where it is closest to 0 in X and Y!\n",
    "\n",
    "#you can click/hold in the graph below to rotate!\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(x=theta1_vals, y=theta0_vals, z=J_vals)])\n",
    "fig.update_layout(title='Loss function for different thetas', autosize=True,\n",
    "                  width=600, height=600, xaxis_title='theta0', \n",
    "                 yaxis_title='theta1')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the same figure as above, with the line the loss function takes toward the minimum.\n",
    "\n",
    "#X = Theta0\n",
    "#Y - Theta1\n",
    "#Z = Loss\n",
    "#black line = path of loss function over the iterations.\n",
    "#Find where it is closest to 0 in X and Y!\n",
    "\n",
    "#you can click/hold in the graph below to rotate!\n",
    "\n",
    "line_marker = dict(color='#101010', width=2)\n",
    "fig = go.Figure()\n",
    "fig.add_surface(x=theta1_vals, y=theta0_vals, z=J_vals)\n",
    "fig.add_scatter3d(x=theta1_history, y=theta0_history, z=J_history, line=line_marker, name='')\n",
    "#The below line adds a graph of just the loss over iterations in a 2D plane\n",
    "plt.plot(theta0_history, theta1_history, 'r+');\n",
    "fig.update_layout(title='Loss function for different thetas', autosize=True,\n",
    "                  width=600, height=600, xaxis_title='theta0', \n",
    "                 yaxis_title='theta1')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerun the optimization above, but this time start closer to the objective!\n",
    "#Find where the black line ends near the lowest X/Y/Z coordinate and make that your guess below.\n",
    "\n",
    "num_iterations=10 #<---start with the same iterations as above\n",
    "theta_init=np.array([[1],[1]]) #<---make a guess as to a more accurate [x],[y] coordinates near the minimum in the graph above.\n",
    "alpha= 0.1 #<---start with the same step size as above\n",
    "theta1, J_history1, theta0_history1, theta1_history1 = gradient_descent(X,y, theta_init,\n",
    "                                                                   alpha, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c593cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the new loss path on the function. It should start much closer to the goal\n",
    "\n",
    "line_marker = dict(color='#101010', width=2)\n",
    "fig = go.Figure()\n",
    "fig.add_surface(x=theta1_vals, y=theta0_vals, z=J_vals)\n",
    "fig.add_scatter3d(x=theta1_history1, y=theta0_history1, z=J_history1, line=line_marker, name='')\n",
    "#The below line adds a graph of just the loss over iterations in a 2D plane\n",
    "plt.plot(theta0_history1, theta1_history1, 'r+');\n",
    "fig.update_layout(title='Loss function for different thetas', autosize=True,\n",
    "                  width=600, height=600, xaxis_title='theta0', \n",
    "                 yaxis_title='theta1')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a76c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This plot shows the convergence similar to above, but only in the X/Y plane (there's no height)\n",
    "\n",
    "plt.contour(theta0_vals, theta1_vals, J_vals, levels = np.logspace(0,10,1000))\n",
    "plt.xlabel('$\\\\theta_{0}$'); plt.ylabel(\"$\\\\theta_{1}$\")\n",
    "plt.title(\"Contour plot of loss function for different values of $\\\\theta$s\");\n",
    "plt.plot(theta0_history1, theta1_history1, 'r+');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How well does gradient descent converge? How much do you need to adjust between different weather stations and years?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
